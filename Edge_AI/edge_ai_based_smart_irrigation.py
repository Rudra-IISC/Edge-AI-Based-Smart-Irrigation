# -*- coding: utf-8 -*-
"""Edge AI Based Smart Irrigation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xs-h7FyT-_sMzaCn5TdOtSBEfkTJMlPL

Edge AI Based Smart Irrigation System

Team Members : Maharudra, Nikhil, Surya
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

url=f"https://raw.githubusercontent.com/Rudra-IISC/Edge-AI-Based-Smart-Irrigation/main/Edge_AI/Output_Data__ET0.csv"
df=pd.read_csv(url)
df.tail()
df.shape
df.head()

def remove_outliers_iqr(df, features):
    cleaned_df = df.copy()
    for feature in features:
        Q1 = cleaned_df[feature].quantile(0.25)
        Q3 = cleaned_df[feature].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        cleaned_df = cleaned_df[(cleaned_df[feature] >= lower) & (cleaned_df[feature] <= upper)]
        print(f"Removed {len(df) - len(cleaned_df)} outliers from {feature}")
    return cleaned_df
features = ['T2M_MIN', 'T2M_MAX', 'RH2M', 'WS2M', 'ALLSKY_SFC_SW_DWN', 'ET0']
df_cleaned= remove_outliers_iqr(df, features)
df_cleaned.shape

!pip install -U ydata_profiling

import ydata_profiling
import pandas as pd # Import pandas for reading the CSV

# Load the data into a DataFrame named 'output_data_with_ET0'
output_data_with_ET0 = pd.read_csv(f"https://raw.githubusercontent.com/Rudra-IISC/Edge-AI-Based-Smart-Irrigation/main/Edge_AI/Output_Data__ET0.csv")

from ydata_profiling.utils.cache import cache_file

report = df_cleaned.profile_report(sort=None, html={"style": {"full_width": True}}, progress_bar=False)
report

profile_report = df_cleaned.profile_report(html={"style": {"full_width": True}})
profile_report.to_file("example.html")

profile_report = df_cleaned.profile_report(
    explorative=True, html={"style": {"full_width": True}})
profile_report

df_cleaned.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(df_cleaned.corr(), annot=True, cmap='coolwarm')

df_cleaned.drop(columns=['YEAR', 'DY', 'MO'], inplace=True)

# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler()
# df_scaled = scaler.fit_transform(df_cleaned)
# df_scaled = pd.DataFrame(df_scaled, columns=df_cleaned.columns)
# df_scaled.head()
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_cleaned)
df_scaled = pd.DataFrame(df_scaled, columns=df_cleaned.columns)
df_scaled.head()

from sklearn.model_selection import train_test_split
X = df_scaled.drop(columns=['ET0'])
y = df_scaled['ET0']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score

model_LR = LinearRegression()
model_LR.fit(X_train, y_train)
y_pred_LR = model_LR.predict(X_test)
mse_LR = mean_squared_error(y_test, y_pred_LR)
r2_LR = r2_score(y_test, y_pred_LR)
print("Linear Regression MSE:", mse_LR)
print("Linear Regression R2:", r2_LR)

best_features = ['T2M_MAX', 'RH2M', 'ALLSKY_SFC_SW_DWN']

df_new = df_scaled[best_features]
df_new['ET0'] = df_scaled['ET0']
df_new.head()

X_train1, X_test1, y_train1, y_test1 = train_test_split(df_new.drop(columns=['ET0']), df_new['ET0'], test_size=0.2, random_state=42)

model_LR1 = LinearRegression()
model_LR.fit(X_train1, y_train1)
y_pred_LR = model_LR.predict(X_test1)
mse_LR = mean_squared_error(y_test1, y_pred_LR)
r2_LR = r2_score(y_test1, y_pred_LR)
print("Linear Regression MSE:", mse_LR)
print("Linear Regression R2:", r2_LR)

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

Model1_SVR_df_new = SVR()
Model1_SVR_df_new.fit(X_train1, y_train1)
y_pred_SVR = Model1_SVR_df_new.predict(X_test1)
mse_SVR = mean_squared_error(y_test1, y_pred_SVR)
r2_SVR = r2_score(y_test1, y_pred_SVR)
print("Support Vector Regressor MSE:", mse_SVR)
print("Support Vector Regressor R2:", r2_SVR)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# --- Function to estimate MLP model size ---
def estimate_mlp_size(model, dtype_size=8):
    """
    Estimate memory size of a trained MLPRegressor in bytes.
    Parameters:
        model: Trained sklearn.neural_network.MLPRegressor model
        dtype_size: Size of each parameter in bytes (default 8 for float64)
    Returns:
        Total size in bytes
    """
    total_params = 0
    for coef in model.coefs_:
        total_params += coef.size
    for intercept in model.intercepts_:
        total_params += intercept.size

    total_size_bytes = total_params * dtype_size
    total_size_kb = total_size_bytes / 1024
    total_size_mb = total_size_kb / 1024

    print(f"MLP Model Parameter Count: {total_params}")
    print(f"Estimated Model Size: {total_size_bytes:.2f} bytes ({total_size_kb:.2f} KB / {total_size_mb:.2f} MB)")
    return total_size_bytes

# --- Data Preparation ---
try:
    df = pd.read_csv('https://raw.githubusercontent.com/Rudra-IISC/Edge-AI-Based-Smart-Irrigation/main/Edge_AI/Output_Data__ET0.csv')

    print("--- Raw Data Head ---")
    print(df.head())
    print("\n--- Raw Data Info ---")
    df.info()
    print("\n")

    best_features = ['T2M_MAX', 'RH2M', 'ALLSKY_SFC_SW_DWN']
    target_variable = 'ET0'
    required_columns = best_features + [target_variable]
    if not all(col in df.columns for col in required_columns):
        missing_cols = [col for col in required_columns if col not in df.columns]
        raise ValueError(f"Missing required columns in CSV: {missing_cols}")

    X = df[best_features].copy()
    y = df[target_variable].copy()

    print("--- Scaling Features ---")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X = pd.DataFrame(X_scaled, columns=best_features, index=X.index)
    print("Features scaled using StandardScaler.\n")

    X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"Training set size: {X_train1.shape[0]} samples")
    print(f"Test set size: {X_test1.shape[0]} samples\n")

    results = {}

    # 1. Linear Regression
    print("--- Training Linear Regression ---")
    model_LR = LinearRegression()
    model_LR.fit(X_train1, y_train1)
    y_pred_LR = model_LR.predict(X_test1)
    mse_LR = mean_squared_error(y_test1, y_pred_LR)
    r2_LR = r2_score(y_test1, y_pred_LR)
    results['Linear Regression'] = {'MSE': mse_LR, 'R2': r2_LR}
    print(f"Linear Regression MSE: {mse_LR:.4f}")
    print(f"Linear Regression R2: {r2_LR:.4f}\n")

    # 2. Support Vector Regressor (SVR)
    print("--- Training Support Vector Regressor (SVR) ---")
    model_SVR = SVR()
    model_SVR.fit(X_train1, y_train1)
    y_pred_SVR = model_SVR.predict(X_test1)
    mse_SVR = mean_squared_error(y_test1, y_pred_SVR)
    r2_SVR = r2_score(y_test1, y_pred_SVR)
    results['SVR'] = {'MSE': mse_SVR, 'R2': r2_SVR}
    print(f"Support Vector Regressor MSE: {mse_SVR:.4f}")
    print(f"Support Vector Regressor R2: {r2_SVR:.4f}\n")

    # 3. Random Forest Regressor
    print("--- Training Random Forest Regressor ---")
    model_RF = RandomForestRegressor(n_estimators=100, random_state=42)
    model_RF.fit(X_train1, y_train1)
    y_pred_RF = model_RF.predict(X_test1)
    mse_RF = mean_squared_error(y_test1, y_pred_RF)
    r2_RF = r2_score(y_test1, y_pred_RF)
    results['Random Forest'] = {'MSE': mse_RF, 'R2': r2_RF}
    print(f"Random Forest Regressor MSE: {mse_RF:.4f}")
    print(f"Random Forest Regressor R2: {r2_RF:.4f}\n")

    # 4. MLP Regressor
    print("--- Training MLP Regressor ---")
    model_MLP = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42, early_stopping=True, n_iter_no_change=10)
    model_MLP.fit(X_train1, y_train1)
    y_pred_MLP = model_MLP.predict(X_test1)
    mse_MLP = mean_squared_error(y_test1, y_pred_MLP)
    r2_MLP = r2_score(y_test1, y_pred_MLP)
    results['MLP Regressor'] = {'MSE': mse_MLP, 'R2': r2_MLP}
    print(f"MLP Regressor MSE: {mse_MLP:.4f}")
    print(f"MLP Regressor R2: {r2_MLP:.4f}")

    print("--- Estimating MLP Regressor Model Size ---")
    original_size_bytes = estimate_mlp_size(model_MLP)

    # Simulated compression techniques:
    # 1. Quantization to 8-bit (1 byte per parameter)
    quantized_size_bytes = original_size_bytes / 8

    # 2. Example pruning - assume 30% of weights are pruned
    pruned_size_bytes = original_size_bytes * 0.7

    print(f"\nSimulated Quantized Model Size (8-bit): {quantized_size_bytes:.2f} bytes ({quantized_size_bytes/1024:.2f} KB)")
    print(f"Simulated Pruned Model Size (30% smaller): {pruned_size_bytes:.2f} bytes ({pruned_size_bytes/1024:.2f} KB)")

    # --- Model Comparison ---
    print("--- Model Comparison Results ---")
    results_df = pd.DataFrame(results).T
    print(results_df)
    print("\n")

    # --- Visualization ---
    print("--- Generating Comparison Plots ---")
    sns.set(style="whitegrid")
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('Model Performance Comparison', fontsize=16)

    mse_sorted = results_df.sort_values('MSE')
    sns.barplot(x=mse_sorted.index, y='MSE', data=mse_sorted, ax=axes[0], palette='viridis')
    axes[0].set_title('Mean Squared Error (MSE)')
    axes[0].set_ylabel('MSE')
    axes[0].set_xlabel('Model')
    axes[0].tick_params(axis='x', rotation=45)

    r2_sorted = results_df.sort_values('R2', ascending=False)
    sns.barplot(x=r2_sorted.index, y='R2', data=r2_sorted, ax=axes[1], palette='viridis')
    axes[1].set_title('R-squared (R2) Score')
    axes[1].set_ylabel('R2 Score')
    axes[1].set_xlabel('Model')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].axhline(0, color='grey', lw=1, linestyle='--')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

except FileNotFoundError:
    print("Error: The file 'output_data_with_ET0.csv' was not found.")
except ValueError as ve:
    print(f"Data Error: {ve}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

print("--- Finished ---")

plt.figure(figsize=(10, 6))
plt.scatter(y_test1, y_pred_MLP, alpha=0.5)
plt.plot([y_test1.min(), y_test1.max()], [y_test1.min(), y_test1.max()], 'r--', lw=2)
plt.xlabel('Actual ET0')
plt.ylabel('Predicted ET0')
plt.title('Actual vs Predicted ET0 (MLP Regressor)')
plt.show()

sns.lineplot(x=y_test1.index, y=y_test1, label='Actual ET0')
sns.lineplot(x=y_test1.index, y=y_pred_MLP, label='Predicted ET0')
plt.xlabel('Index')
plt.ylabel('ET0')
plt.title('Actual vs Predicted ET0 Over Time (MLP Regressor)')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
import warnings

# Suppress ConvergenceWarning for cleaner output during extraction focus
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# --- Configuration ---
CSV_FILE = 'https://raw.githubusercontent.com/Rudra-IISC/Edge-AI-Based-Smart-Irrigation/main/Edge_AI/Output_Data__ET0.csv'
BEST_FEATURES = ['T2M_MAX', 'RH2M', 'ALLSKY_SFC_SW_DWN']
TARGET_VARIABLE = 'ET0'
TEST_SIZE = 0.2
RANDOM_STATE = 42
# MLP Parameters (Keep it relatively simple for Pico)
# You might need to experiment with smaller layers if memory is still an issue
HIDDEN_LAYER_SIZES = (16, 8) # Reduced layers for Pico suitability
MAX_ITER = 1500 # Increased iterations might be needed for smaller networks
EARLY_STOPPING = True
N_ITER_NO_CHANGE = 15

# --- Data Preparation ---
print(f"--- Loading Data from {CSV_FILE} ---")
try:
    df = pd.read_csv(CSV_FILE)
    print("Data loaded successfully.")

    # Check for required columns
    required_columns = BEST_FEATURES + [TARGET_VARIABLE]
    if not all(col in df.columns for col in required_columns):
        missing_cols = [col for col in required_columns if col not in df.columns]
        raise ValueError(f"Missing required columns in CSV: {missing_cols}")

    X = df[BEST_FEATURES].copy()
    y = df[TARGET_VARIABLE].copy()

    # Handle potential NaN values (replace with mean, or choose another strategy)
    if X.isnull().values.any():
        print("Warning: NaN values found in features. Filling with mean.")
        X = X.fillna(X.mean())
    if y.isnull().values.any():
        print("Warning: NaN values found in target. Filling with mean.")
        y = y.fillna(y.mean())


    # --- Feature Scaling ---
    print("--- Scaling Features ---")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print("Features scaled using StandardScaler.")
    # Keep X_scaled as numpy array for training

    # --- Train/Test Split ---
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )
    print(f"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\n")

    # --- Train MLP Regressor ---
    print(f"--- Training MLP Regressor {HIDDEN_LAYER_SIZES} ---")
    model_MLP = MLPRegressor(
        hidden_layer_sizes=HIDDEN_LAYER_SIZES,
        activation='relu', # Standard activation, easy to implement
        solver='adam',
        max_iter=MAX_ITER,
        random_state=RANDOM_STATE,
        early_stopping=EARLY_STOPPING,
        n_iter_no_change=N_ITER_NO_CHANGE,
        learning_rate_init=0.001 # Default, adjust if needed
    )
    model_MLP.fit(X_train, y_train)
    print("MLP Model training complete.")

    # --- Evaluate (Optional but recommended) ---
    from sklearn.metrics import mean_squared_error, r2_score
    y_pred_MLP = model_MLP.predict(X_test)
    mse_MLP = mean_squared_error(y_test, y_pred_MLP)
    r2_MLP = r2_score(y_test, y_pred_MLP)
    print(f"\n--- Model Evaluation ---")
    print(f"MLP Regressor MSE: {mse_MLP:.4f}")
    print(f"MLP Regressor R2: {r2_MLP:.4f}\n")


    # --- Extract Parameters ---
    print("--- Extracting Model Parameters for MicroPython ---")

    # 1. Scaler Parameters
    scaler_mean = scaler.mean_.tolist()
    scaler_scale = scaler.scale_.tolist() # Use scale_ (standard deviation)

    # 2. MLP Weights and Biases
    # Coefs are weights, Intercepts are biases
    weights = [coef.tolist() for coef in model_MLP.coefs_]
    biases = [intercept.tolist() for intercept in model_MLP.intercepts_]

    # --- Print Parameters in MicroPython Format ---
    # Clear instructions for the user
    print("\n" + "="*50)
    print("COPY THE FOLLOWING PARAMETERS INTO YOUR MicroPython SCRIPT")
    print("="*50 + "\n")

    print("# --- Scaler Parameters ---")
    print(f"SCALER_MEAN = {scaler_mean}")
    print(f"SCALER_SCALE = {scaler_scale}\n")

    print("# --- MLP Parameters ---")
    # Print weights layer by layer
    for i, w in enumerate(weights):
        print(f"# Weights: Layer {i} (Input/Previous Layer -> Layer {i+1})")
        print(f"WEIGHTS_{i} = [")
        for row in w:
            print(f"    {row},")
        print("]\n")

    # Print biases layer by layer
    for i, b in enumerate(biases):
        print(f"# Biases: Layer {i+1}")
        print(f"BIASES_{i} = {b}\n")

    print("="*50)
    print("PARAMETER EXTRACTION COMPLETE")
    print("="*50)


except FileNotFoundError:
    print(f"Error: The file '{CSV_FILE}' was not found.")
except ValueError as ve:
    print(f"Data Error: {ve}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

import numpy as np
from sklearn.neural_network import MLPRegressor

def estimate_mlp_size(model: MLPRegressor, dtype_size=8):
    """
    Estimate memory size of a trained MLPRegressor in bytes.

    Parameters:
        model: Trained sklearn.neural_network.MLPRegressor model
        dtype_size: Size of each parameter in bytes (default 8 for float64)

    Returns:
        Total size in bytes
    """
    total_params = 0

    for coef in model.coefs_:
        total_params += coef.size
    for intercept in model.intercepts_:
        total_params += intercept.size

    total_size_bytes = total_params * dtype_size
    total_size_kb = total_size_bytes / 1024
    total_size_mb = total_size_kb / 1024

    print(f"Total parameters: {total_params}")
    print(f"Estimated size: {total_size_bytes:.2f} bytes ({total_size_kb:.2f} KB / {total_size_mb:.2f} MB)")
    return total_size_bytes

# Example usage
# model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500).fit(X_train, y_train)
# estimate_mlp_size(model)

